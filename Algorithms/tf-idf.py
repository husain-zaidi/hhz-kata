"""
TF-IDF Search Demo (generated by gpt4.1) human commentary)
This script demonstrates TF-IDF vectorization and search over a set of sample documents, implemented from scratch.
"""
import numpy as np
import math
import re
from collections import Counter, defaultdict

# Sample documents
documents = [
    "The quick brown fox jumps over the lazy dog.",
    "Never jump over the lazy dog quickly.",
    "A fast brown fox leaps over sleeping dogs.",
    "Python and Java are popular programming languages.",
    "The dog is lazy but the fox is quick."
]

doc_titles = [
    "Doc 1: Fox and Dog",
    "Doc 2: Jumping Dog",
    "Doc 3: Fast Fox",
    "Doc 4: Programming Languages",
    "Doc 5: Lazy Dog and Quick Fox"
]

def tokenize(text):
    # Lowercase and split on non-word characters
    return re.findall(r'\b\w+\b', text.lower())

# Build vocabulary
vocab = set()
doc_tokens = [] # list of all document split into tokens
for doc in documents:
    tokens = tokenize(doc)
    doc_tokens.append(tokens)
    vocab.update(tokens)
vocab = sorted(vocab)
word2idx = {word: i for i, word in enumerate(vocab)}

# Compute term frequency (TF) of a document
# TF is the number of times a word appears in a document divided by the total number of words in that document
def compute_tf(tokens):
    tf = np.zeros(len(vocab))
    counts = Counter(tokens)
    for word, count in counts.items():
        idx = word2idx[word]
        tf[idx] = count / len(tokens)
    return tf

tf_matrix = np.array([compute_tf(tokens) for tokens in doc_tokens])

# Compute document frequency (DF)
# DF is the number of documents containing the word
df = np.zeros(len(vocab))
for i, word in enumerate(vocab):
    df[i] = sum(1 for tokens in doc_tokens if word in tokens)

# Compute inverse document frequency (IDF)
#  its like the importance of the word in the corpus
idf = np.log((1 + len(documents)) / (1 + df)) + 1  # Smoothing

# Compute TF-IDF matrix
# every document is represented as a vector of TF-IDF scores
tfidf_matrix = tf_matrix * idf

def vectorize_query(query):
    tokens = tokenize(query)
    tf = compute_tf(tokens)
    return tf * idf

def cosine_similarity(vec1, vec2):
    norm1 = np.linalg.norm(vec1)
    norm2 = np.linalg.norm(vec2)
    if norm1 == 0 or norm2 == 0:
        return 0.0
    return np.dot(vec1, vec2) / (norm1 * norm2)

def search(query, top_n=3):
    query_vec = vectorize_query(query)
    scores = [cosine_similarity(doc_vec, query_vec) for doc_vec in tfidf_matrix]
    ranked_indices = np.argsort(scores)[::-1]
    print(f"\nSearch results for query: '{query}'\n{'='*40}")
    for idx in ranked_indices[:top_n]:
        print(f"{doc_titles[idx]} (Score: {scores[idx]:.4f})\n  {documents[idx]}\n")

# Demonstration
if __name__ == "__main__":
    # Example queries
    search("quick fox")
    search("programming languages")
    search("lazy dog")